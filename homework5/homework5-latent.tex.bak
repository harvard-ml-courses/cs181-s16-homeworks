\documentclass[submit]{harvardml}

% Put in your full name and email address.
\name{Yohann Smadja}
\email{smadja@post.harvard.edu}

% List any people you worked with.
%\collaborators{%
%  John Doe,
%  Fred Doe
%}

% You don't need to change these.
\course{CS181-S16}
\assignment{Assignment \#5}
\duedate{5:00pm April 15, 2016}

\usepackage[OT1]{fontenc}
\usepackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage[pdftex]{graphicx}
\usepackage{subfig}
\usepackage{fullpage}
\usepackage{palatino}
\usepackage{mathpazo}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{color}
\usepackage{todonotes}
\usepackage{listings}
\usepackage{common}
\usepackage{bm}
\usepackage{enumitem}

\usepackage[mmddyyyy,hhmmss]{datetime}

\definecolor{verbgray}{gray}{0.9}

\lstnewenvironment{csv}{%
  \lstset{backgroundcolor=\color{verbgray},
  frame=single,
  framerule=0pt,
  basicstyle=\ttfamily,
  columns=fullflexible}}{}

\begin{document}
\begin{center}
{\Large Homework 5: EM for a Simple Topic Model}\\
\end{center}

There is a mathematical component and a programming component to this homework.
Please submit ONLY your PDF to Canvas, and push all of your work to your Github
repository. If a question requires you to make any plots, please
include those in the writeup.

\begin{mdframed}[style=exampledefault]
\textbf{Background:} In this homework, you will implement a very simple kind of topic model.  Latent Dirichlet allocation, as we discussed in class, is a topic model in which each document is composed of multiple topics.  Here we will make a simplified version in which each document has just a single topic.  As in LDA, the vocabulary will have~$V$ words and a topic will be a distribution over this vocabulary.  Let's use~$K$ topics and the~$k$th topic is a vector~$\bbeta_k$, where~${\beta_{k,v}\geq 0}$ and~${\sum_v \beta_{k,v}=1}$.  Each document can be described by a set of word counts~$\boldw_{d}$, where~$w_{d,v}$ is a nonnegative integer.  Document~$d$ has~$N_d$ words in total, i.e.,~${\sum_v w_{d,v}=N_d}$.  Let's have the unknown overall mixing proportion of topics be~$\btheta$, where~${\theta_k\geq 0}$ and~${\sum_k\theta_k=1}$.  Our generative model is that each of the~$D$ documents has a single topic~${z_d\in \{1,\ldots,K\}}$, drawn from~$\btheta$; then, each of the words is drawn from~$\bbeta_{z_d}$.

\end{mdframed}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[Complete Data Log Likelihood, 4 pts]

Write the complete-data log likelihood~$\ln p(\{z_d,\boldw_d\}^D_{d=1}\given\btheta, \{\bbeta_k\}^K_{k=1})$. It may be convenient to write~$z_d$ as a one-hot coded vector~$\boldz_d$.


\end{problem}
\subsection*{Solution}

$ \displaystyle \ln p(\{z_d,\boldw_d\}^D_{d=1}\given\btheta, \{\bbeta_k\}^K_{k=1}) = \ln \prod_{d=1}^D  p(z_d,\boldw_d \given\btheta, \{\bbeta_k\}^K_{k=1})$\\

since $z_d$ follows a $Categorical_K(\btheta_k)$, we get\\

$ \displaystyle \ln p(\{z_d,\boldw_d\}^D_{d=1}\given\btheta, \{\bbeta_k\}^K_{k=1}) = \ln \prod_{d=1}^D \prod_{k=1}^K \Big[\btheta_k p(\boldw_d \given \{\bbeta_k\}^K_{k=1}) \Big]^{z_{dk}}  $\\

since $w_d$ follows a $Multinomial_{N_d}(\beta_{z_{dk}})$, we get\\

$ \displaystyle \ln p(\{z_d,\boldw_d\}^D_{d=1}\given\btheta, \{\bbeta_k\}^K_{k=1}) = \ln \prod_{d=1}^D \prod_{k=1}^K \Big[\btheta_k  \frac{\Gamma(N_d+1)}{\prod_i \Gamma(w_{id}+1)}  \prod_{i=1}^{N_d} \big( \beta_{ki}^{w_{id}} \big)  \Big]^{z_{dk}}$\\

$ \displaystyle \ln p(\{z_d,\boldw_d\}^D_{d=1}\given\btheta, \{\bbeta_k\}^K_{k=1}) = \sum_{d=1}^D \sum_{k=1}^K z_{dk} \ln \Big[\btheta_k \frac{\Gamma(N_d+1)}{\prod_i \Gamma(w_{id}+1)} \prod_{i=1}^{N_d} \big( \beta_{ki}^{w_{id}} \big)  \Big]$\\

we finally get:\\

$ \displaystyle \ln p(\{z_d,\boldw_d\}^D_{d=1}\given\btheta, \{\bbeta_k\}^K_{k=1}) = \sum_{d=1}^D \sum_{k=1}^K z_{dk} \Big[ \ln \btheta_k + \ln \Gamma(N_d+1) - \sum_{i=1}^{N_d} \ln \Gamma(w_{id}+1) + \sum_{i=1}^{N_d} w_{vd} \ln \beta_{ki} \Big]$\\



\newpage
\begin{problem}[Expectation Step, 5pts]

Introduce estimates~$q(\boldz_d)$ for the posterior over the hidden variables~$\boldz_d$.  What did you choose and why?  Write down how you would determine the parameters of these estimates, given the observed data~$\{\boldw_d\}^D_{d=1}$ and the parameters~$\btheta$ and~$\{\bbeta_k\}^K_{k=1}$.

\end{problem}
\subsection*{Solution}

We have $ \displaystyle q(z_{kd}) \propto  p(z_{kd}=1 \given \theta) \prod_{i=1}^{N_d} p(w_{id} \given z_{kd}=1,\beta_{k=1..K})$\\

It's simply the prior times the likelihood.\\

We take $p(z_{kd}=1 \given \theta)=\theta_k$ \\

$ \displaystyle q(z_{kd}) \propto \theta_k \prod_{i=1}^{N_d} p(w_{id} \given z_{kd}=1,\beta_{k=1..K})$\\

$w_{d}$ is drawn from a $Multinomial(\beta_k)$:\\

$ \displaystyle q(z_{kd}) \propto \theta_k \prod_{i=1}^{N_d} \beta_{ki}^{w_{id}}$\\

We then normalize and get: \\


$$ \displaystyle q(z_{d}) = \Bigg( \frac{\theta_k \prod_{i=1}^{N_d} \beta_{ki}^{w_{id}}}{\sum_{k=1}^K \theta_k \prod_{i=1}^{N_d} \beta_{ki}^{w_{id}}} \Bigg)_{k=1...K}$$





\newpage

\begin{problem}[Maximization Step, 5pts]
With the~$q(\boldz_d)$ estimates in hand from the E-step, derive an update for maximizing the expected complete data log likelihood in terms of~$\btheta$ and~$\{\bbeta_k\}^K_{k=1}$.

\begin{enumerate}[label=(\alph*)]
    \item Derive an expression for the expected complete data log likelihood for fixed $\gamma$'s.
    \item Find a value of $\theta$ that maximizes the expected complete data log likelihood derived in (a). You may find it helpful to use Lagrange multipliers in order to force the constraint $\sum \theta_k = 1$. Why does this optimized $\theta$ make intuitive sense?
    \item Apply a similar argument to find the value of $\beta_{k, v}$ that maximizes the expected complete data log likelihood.
\end{enumerate}

\end{problem}
\subsection*{Solution}

a- In this 'Maximization' step, we suppose $\gamma$ given (from the 'Expectation' step) and fixed\\

$\displaystyle  \mathbb{E}_z \big[ \ln p(\{z_d,\boldw_d\}^D_{d=1}\given\btheta, \{\bbeta_k\}^K_{k=1}) \big]= \sum_{d=1}^D \sum_{k=1}^K \gamma_{dk} \ln p(\{z_d,\boldw_d\}\given\btheta, \{\bbeta_k\}^K_{k=1}) $\\


$\displaystyle  \mathbb{E}_z \big[ \log p(\{z_d,\boldw_d\}^D_{d=1}\given\btheta, \{\bbeta_k\}^K_{k=1}) \big]= \sum_{d=1}^D \sum_{k=1}^K \gamma_{dk} \Big[ \ln \theta_k + \sum_{v=1}^V w_{vd} \ln \beta_{kv} \Big] $\\


b - We derive the previous expression by $\theta_k$ including the Lagrangian from the constraint: $\sum \theta_k = 1$ \\

$\displaystyle  \frac{\partial}{\partial \theta_k} \sum_{d=1}^D \sum_{k=1}^K \gamma_{dk} \Big[ \ln \theta_k + \sum_{v=1}^V w_{vd} \ln \beta_{kv} +\lambda (1-\sum_{k=1}^K \theta_k )\Big]=0$\\

$\displaystyle  \frac{\partial}{\partial \theta_k} \sum_{d=1}^D \sum_{k=1}^K \gamma_{dk} \Big[ \ln \theta_k +\lambda (1-\sum_{k'=1}^K \theta_k' )\Big]=0$\\

This implies:\\

$\displaystyle \sum_{d=1}^D \gamma_{dk} \big( \frac{1}{\theta_k} -\lambda \big)=0$\\

$\displaystyle \sum_{d=1}^D \gamma_{dk} \big( 1-\lambda \theta_k \big)=0$\\

$\displaystyle \sum_{d=1}^D \gamma_{dk} = \sum_{d=1}^D \lambda \theta_k $\\

$\displaystyle \sum_{d=1}^D \gamma_{dk} =  \lambda D \theta_k $\\

hence, we get the following equation (1): \\

$\displaystyle \theta_k = \frac{1}{\lambda D}\sum_{d=1}^D \gamma_{dk} $\\

Now let's sum over $k$:\\

$\displaystyle \sum_{k=1}^K \theta_k =  \frac{1}{\lambda D} \sum_{k=1}^K \sum_{d=1}^D \gamma_{dk} $\\

We know that $\displaystyle \sum_{k=1}^K \theta_k = 1$ and $\displaystyle \sum_{k=1}^K \gamma_{dk} = \sum_{k=1}^K q(z_{dk})=1$ so $\displaystyle \sum_{k=1}^K \sum_{d=1}^D \gamma_{dk} = D $\\

We now have $\displaystyle 1 =  \frac{D}{\lambda D} $ so $\lambda = 1$\\

Replacing $\lambda$ in equation (1), we get:\\

$\displaystyle \theta_k= \frac{1}{D} \sum_{d=1}^D \gamma_{dk} $\\

which makes a lot of sense, $\theta_k$ becomes the average probability of documents being of topic $k$.\\


c- We want to derive the expected log likelihood with respect to $\beta_{kv}$.\\

We add the Lagrangian for the constraint: $\sum_{v=1}^V \beta_{kv}=1$ \\

$\displaystyle \frac{\partial}{\partial \beta_{kv}} \Big[ \sum_{d=1}^D \sum_{k=1}^K \gamma_{dk} \Big( \ln \theta_k + \sum_{v=1}^V w_{vw} \ln \beta_{kv} +\lambda (1-\sum_{v=1}^V \beta_{kv} ) \Big) \Big] = 0$\\

$\displaystyle \sum_{d=1}^D \gamma_{dk} (\frac{w_{vd}}{\beta_{kv}}-\lambda)=0   $\\

$\displaystyle \sum_{d=1}^D \gamma_{dk} (w_{vd}-\lambda\beta_{kv})=0   $\\

$\displaystyle \sum_{d=1}^D \gamma_{dk}w_{vd}= \sum_{d=1}^D \lambda \beta_{kv}   $\\

$\displaystyle \sum_{d=1}^D \gamma_{dk}w_{vd}= \lambda D \beta_{kv}   $\\

We get the following equation (2):\\

$\beta_{kv}  = \frac{1}{\lambda D} \sum_{d=1}^D \gamma_{dk} w_{vd}$\\

Let's sum over $v$:\\

$\displaystyle \sum_{v=1}^V \beta_{kv}  = \frac{1}{\lambda D} \sum_{v=1}^V \sum_{d=1}^D \gamma_{dk} w_{vd}$\\

We know that $\displaystyle \sum_{v=1}^V \beta_{kv}=1$:\\

$\displaystyle \sum_{v=1}^V \sum_{d=1}^D \gamma_{dk} w_{vd} = \lambda D $\\

$\displaystyle  \sum_{d=1}^D \gamma_{dk} \Big( \sum_{v=1}^V w_{vd} \Big) = \lambda D $\\

By definition $\sum_{v=1}^V w_{vd} = N_d$\\

$\displaystyle  \lambda = \frac{1}{D} \sum_{d=1}^D \gamma_{dk} N_d$\\

We plug this into equation (2):\\

$\displaystyle \hat{\beta}_{kv} = \frac{D}{\sum_{d=1}^D \gamma_{dk} N_d} \frac{1}{D} \sum_{d=1}^D \gamma_{dk} w_{vd} $\\

Finally,\\

$\displaystyle \hat{\beta}_{kv} = \frac{\sum_{d=1}^D \gamma_{dk} w_{vd} }{\sum_{d=1}^D \gamma_{dk} N_d} $\\



\newpage


\begin{problem}[Implementation, 10pts]
Implement this expectation maximization algorithm and try it out on some text data.  In order for the EM algorithm to work, you may have to do a little preprocessing.

$\linebreak$
\noindent The starter code loads the text data as a numpy array that is $5224951 \times 3$ in size. As shown below, the first number in the numpy array represents the document\_id, the second number represents a word\_id, and the third number is the count the word appears.


$$ [\text{doc\_id, word\_id, count}]$$

\noindent A dictionary of the mappings between word\_ids and words is also provided. The full dataset description can be found at \url{http://kdd.ics.uci.edu/databases/nsfabs/nsfawards.data.html}.\\



\noindent Plot the objective function as a function of iteration and verify that it never increases. Try different numbers of topics and report what topics you find by, e.g., listing the most likely words.



\end{problem}
\subsection*{Solution}

After an extensive use of Pandas' pivot\_table and merge functions, I was able to get an iteration (step E, step M and computation of the expected complete data log likelihood) under 15 seconds. \\

I have initialized theta and beta randomly. My loss function converges and never increases but unfortunately I have noticed that after a few iterations my matrix beta converges towards a matrix where all $beta_v$ are the same for all $k$. Which means that the most 'likely' word of a topic is always the same one. \\

\begin{figure}[htb]
\begin{center}
\includegraphics[width=8cm]{optimization.png}
\caption{Loss function vs iteration}
\end{center}
\end{figure}


\newpage
\begin{problem}[Calibration, 1pt]
Approximately how long did this homework take you to complete? 24h
\end{problem}


\end{document}
