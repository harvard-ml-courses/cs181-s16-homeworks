\documentclass[submit]{harvardml}

% Put in your full name and email address.
\name{Yohann Smadja}
\email{smadja@post.harvard.edu}

% List any people you worked with.
%\collaborators{%
%  John Doe,
%  Fred Doe
%}

% You don't need to change these.
\course{CS181-S16}
\assignment{Assignment \#5}
\duedate{5:00pm April 15, 2016}

\usepackage[OT1]{fontenc}
\usepackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage[pdftex]{graphicx}
\usepackage{subfig}
\usepackage{fullpage}
\usepackage{palatino}
\usepackage{mathpazo}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{color}
\usepackage{todonotes}
\usepackage{listings}
\usepackage{common}
\usepackage{bm}
\usepackage{enumitem}

\usepackage[mmddyyyy,hhmmss]{datetime}

\definecolor{verbgray}{gray}{0.9}

\lstnewenvironment{csv}{%
  \lstset{backgroundcolor=\color{verbgray},
  frame=single,
  framerule=0pt,
  basicstyle=\ttfamily,
  columns=fullflexible}}{}

\begin{document}
\begin{center}
{\Large Homework 5: EM for a Simple Topic Model}\\
\end{center}

There is a mathematical component and a programming component to this homework.
Please submit ONLY your PDF to Canvas, and push all of your work to your Github
repository. If a question requires you to make any plots, please
include those in the writeup.

\begin{mdframed}[style=exampledefault]
\textbf{Background:} In this homework, you will implement a very simple kind of topic model.  Latent Dirichlet allocation, as we discussed in class, is a topic model in which each document is composed of multiple topics.  Here we will make a simplified version in which each document has just a single topic.  As in LDA, the vocabulary will have~$V$ words and a topic will be a distribution over this vocabulary.  Let's use~$K$ topics and the~$k$th topic is a vector~$\bbeta_k$, where~${\beta_{k,v}\geq 0}$ and~${\sum_v \beta_{k,v}=1}$.  Each document can be described by a set of word counts~$\boldw_{d}$, where~$w_{d,v}$ is a nonnegative integer.  Document~$d$ has~$N_d$ words in total, i.e.,~${\sum_v w_{d,v}=N_d}$.  Let's have the unknown overall mixing proportion of topics be~$\btheta$, where~${\theta_k\geq 0}$ and~${\sum_k\theta_k=1}$.  Our generative model is that each of the~$D$ documents has a single topic~${z_d\in \{1,\ldots,K\}}$, drawn from~$\btheta$; then, each of the words is drawn from~$\bbeta_{z_d}$.

\end{mdframed}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[Complete Data Log Likelihood, 4 pts]

Write the complete-data log likelihood~$\ln p(\{z_d,\boldw_d\}^D_{d=1}\given\btheta, \{\bbeta_k\}^K_{k=1})$. It may be convenient to write~$z_d$ as a one-hot coded vector~$\boldz_d$.


\end{problem}
\subsection*{Solution}

$ \displaystyle \ln p(\{z_d,\boldw_d\}^D_{d=1}\given\btheta, \{\bbeta_k\}^K_{k=1}) = \ln \prod_{d=1}^D  p(z_d,\boldw_d \given\btheta, \{\bbeta_k\}^K_{k=1})$\\

since $z_d$ follows a $Categorical_K(\btheta_k)$, we get\\

$ \displaystyle \ln p(\{z_d,\boldw_d\}^D_{d=1}\given\btheta, \{\bbeta_k\}^K_{k=1}) = \ln \prod_{d=1}^D \prod_{k=1}^K \Big[\btheta_k p(\boldw_d \given \{\bbeta_k\}^K_{k=1}) \Big]^{z_{dk}}  $\\

since $w_d$ follows a $Categorical_V(z_{dk})$, we get\\

$ \displaystyle \ln p(\{z_d,\boldw_d\}^D_{d=1}\given\btheta, \{\bbeta_k\}^K_{k=1}) = \ln \prod_{d=1}^D \prod_{k=1}^K \Big[\btheta_k \prod_{v=1}^V \big( \beta_{kv}^{w_{vd}} \big)  \Big]^{z_{dk}}$\\

$ \displaystyle \ln p(\{z_d,\boldw_d\}^D_{d=1}\given\btheta, \{\bbeta_k\}^K_{k=1}) = \sum_{d=1}^D \sum_{k=1}^K z_{dk} \ln \Big[\btheta_k \prod_{v=1}^V \big( \beta_{kv}^{w_{vd}} \big)  \Big]$\\

we finally get:\\

$ \displaystyle \ln p(\{z_d,\boldw_d\}^D_{d=1}\given\btheta, \{\bbeta_k\}^K_{k=1}) = \sum_{d=1}^D \sum_{k=1}^K z_{dk} \Big[ \ln \btheta_k + \sum_{v=1}^V w_{vd} \ln \beta_{kv} \Big]$\\



\newpage
\begin{problem}[Expectation Step, 5pts]

Introduce estimates~$q(\boldz_d)$ for the posterior over the hidden variables~$\boldz_d$.  What did you choose and why?  Write down how you would determine the parameters of these estimates, given the observed data~$\{\boldw_d\}^D_{d=1}$ and the parameters~$\btheta$ and~$\{\bbeta_k\}^K_{k=1}$.

\end{problem}
\subsection*{Solution}

$z_d$ is supposed to be drawn from $\btheta_k$ hence $\btheta$ will be our prior distribution.\\

By matching the counts of words $w_d$ and the distribution of those words by topic, we get the likelihood that the topic of document $d$ is $k$.

$ \displaystyle \hat{z}_{dk} = \sum_{v=1}^V w_{dv}.\beta_{kv}/N_d = w_d^T.\beta_k /N_d $\\

By dividing by $N_d$ we get $ \displaystyle \sum_{k=1}^K \hat{z}_{dk}=1$. $w_d$ and $\beta_k$ are vectors of dimension $V$.\\

The probability that the topic of document $d$ is $k$ increases when words associated with a high probability $\beta_{kv}$ have also a high count.\\

We have $q(z_d) \propto likelihood \times prior$, so we have the following estimator of the posterior:\\

$$  q(z_d) = \Bigg( \frac{\btheta_k w_d^T.\beta_k}{\sum_{k=1}^K \btheta_k w_d^T.\beta_k} \Bigg)_{k=1...K}$$ 




\newpage

\begin{problem}[Maximization Step, 5pts]
With the~$q(\boldz_d)$ estimates in hand from the E-step, derive an update for maximizing the expected complete data log likelihood in terms of~$\btheta$ and~$\{\bbeta_k\}^K_{k=1}$.

\begin{enumerate}[label=(\alph*)]
    \item Derive an expression for the expected complete data log likelihood for fixed $\gamma$'s.
    \item Find a value of $\theta$ that maximizes the expected complete data log likelihood derived in (a). You may find it helpful to use Lagrange multipliers in order to force the constraint $\sum \theta_k = 1$. Why does this optimized $\theta$ make intuitive sense?
    \item Apply a similar argument to find the value of $\beta_{k, v}$ that maximizes the expected complete data log likelihood.
\end{enumerate}

\end{problem}
\subsection*{Solution}

\newpage


\begin{problem}[Implementation, 10pts]
Implement this expectation maximization algorithm and try it out on some text data.  In order for the EM algorithm to work, you may have to do a little preprocessing.

$\linebreak$
\noindent The starter code loads the text data as a numpy array that is $5224951 \times 3$ in size. As shown below, the first number in the numpy array represents the document\_id, the second number represents a word\_id, and the third number is the count the word appears.


$$ [\text{doc\_id, word\_id, count}]$$

\noindent A dictionary of the mappings between word\_ids and words is also provided. The full dataset description can be found at \url{http://kdd.ics.uci.edu/databases/nsfabs/nsfawards.data.html}.\\



\noindent Plot the objective function as a function of iteration and verify that it never increases. Try different numbers of topics and report what topics you find by, e.g., listing the most likely words.



\end{problem}
\subsection*{Solution}

\newpage
\begin{problem}[Calibration, 1pt]
Approximately how long did this homework take you to complete?
\end{problem}


\end{document}
