
\documentclass[submit]{harvardml}

% Put in your full name and email address.
\name{Yohann Smadja}
\email{smadja@post.harvard.edu}

% You don't need to change these.
\course{CS181-S16}
\assignment{Assignment \#4}
\duedate{5:00pm Feb 1, 2016}

\usepackage[OT1]{fontenc}
\usepackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
%\usepackage[pdftex]{graphicx}
\usepackage{subfig}
\usepackage{fullpage}
\usepackage{palatino}
\usepackage{mathpazo}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{color}
\usepackage{todonotes}
\usepackage{listings}
\usepackage{common}
\usepackage{bm}
\usepackage[demo]{graphicx}
\usepackage{caption}
\usepackage{subcaption}


\usepackage[mmddyyyy,hhmmss]{datetime}

\definecolor{verbgray}{gray}{0.9}

\lstnewenvironment{csv}{%
  \lstset{backgroundcolor=\color{verbgray},
  frame=single,
  framerule=0pt,
  basicstyle=\ttfamily,
  columns=fullflexible}}{}

\begin{document}
\begin{center}
{\Large Homework 4: Clustering}\\
\end{center}

There is a mathematical component and a programming component to this homework.
Please submit ONLY your PDF to Canvas, and push all of your work to your Github
repository. If a question requires you to make any plots, please
include those in the writeup.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[The Curse of Dimensionality, 4pts]
In~$d$ dimensions, consider a hypersphere of unit radius, centered at zero,
which is inscribed in a hypercube, also centered at zero, with edges of length
two.  What fraction of the hypercube's volume is contained within the
hypersphere?  Write this as a function of~$d$.  What happens when~$d$ becomes
large?
\end{problem}
\subsection*{Solution}

volume hypersphere = $V_s=\frac{\pi^{d/2}.radius^d}{\Gamma(d/2+1)}$\\

volume hypercube = $V_c=length^d=2^d$\\

with $radius=1$ and $length=2$\\

$\frac{V_s}{V_c}=\frac{\pi^{d/2}}{2^d.\Gamma(d/2+1)}$\\

We can see that the limit of this ratio goes to 0 when $d \rightarrow + \infty$. The volume of the hypercube goes to infinity while the hypersphere converges to 0.\\

\begin{figure}[htb]
\begin{center}
\includegraphics[width=10cm]{problem1.png}
\caption{Ratio of the volumes of unit hypersphere and hypercube of length 2 up to the dimension 15}
\end{center}
\end{figure}

\begin{figure}[htb]
\begin{center}
\includegraphics[width=8cm]{problem11.png}
\caption{Volume of the unit hypersphere up to the dimension 15}
\end{center}
\end{figure}

\begin{figure}[htb]
\begin{center}
\includegraphics[width=8cm]{problem12.png}
\caption{Volume of the hypercube of length 2 up to the dimension 15}
\end{center}
\end{figure}


\newpage
\begin{problem}[Norms, Distances, and Hierarchical Clustering, 5 pts]

  Consider the following four data points, belonging to three clusters: the
  black cluster $((x_1, y_1) = (0.1, 0.5) $ and $(x_2, y_2) = (0.35, 0.75))$,
  the red cluster $(x_3, y_3) = (0.28, 1.35)$ cluster, and the blue cluster
  $(x_4, y_4) = (0, 1.01)$.

  \begin{center} \includegraphics[scale=.4]{scatterplot.png} \end{center}
  At each step of hierarchical clustering, the two most similar (or least
  dissimilar) clusters are merged together. This step is repeated until there is
  one single group. Different distances can be used to measure group
  dissimilarity. Recall the definition of the $l_1$, $l_2$, and $l_{\infty}$
  norm:
  \begin{itemize}
    \item For $\mathbf{x} \in \mathbb{R}^n, \| \mathbf{x} \|_1 = \sum_{i = 1}^n
      |x_i|$
    \item For $\mathbf{x} \in \mathbb{R}^n, \| \mathbf{x} \|_2 = \sqrt{\sum_{i =
      1}^n x_i^2 }$
    \item For $\mathbf{x} \in \mathbb{R}^n, \| \mathbf{x} \|_{\infty} = \max_{i
      = 1}^n |x_i|$
  \end{itemize}
  Also recall the definition of single-link distance, complete-link distance,
  and average-link distance between two clusters:
  \begin{itemize}
    \item Single-link clustering: for clusters $G$ and $H$, $d_{S}(G, H) =
    \min_{i \in G, j\in H} d(i, j)$
    \item Complete-link clustering: for clusters $G$ and $H$, $d_{C}(G, H) =
    \max_{i \in G, j\in H} d(i, j)$
    \item Average-link clustering: for clusters $G$ and $H$, $d_{A}(G, H) =
      \frac{1}{|G| |H|} \sum_{i\in G}\sum_{j \in H} d(i, j)$
  \end{itemize}
  \paragraph{Warm up question.} \noindent Draw the 2D unit sphere for each norm,
  defined as $\mathcal{S} = \{x \in \mathbb{R}^2: \|x\| = 1 \}$. Feel free to do
  it by hand, take a picture and include it in your pdf.

  \paragraph{Main question.}
  \noindent For each norm ($l_1, l_2, l_\infty$) and each clustering method
  (single, complete, or average link clustering), specify which 2 clusters would
  be the first to merge.
\end{problem}
\subsection*{Solution}

1 - We start by drawing 3 unit spheres with the 3 different norms $l_1, l_2, l_\infty$. The norm $l_1$ has the shape of a diamond, the $l_2$ sphere is a circle and finally, the sphere of $l_\infty$ has the shape of a square.\\

\begin{figure}[htb]
\begin{center}
\includegraphics[width=8cm]{hw4pb2q1.png}
\caption{Unit spheres for norms $l_1, l_2, l_\infty$}
\end{center}
\end{figure}


2- Below is the plot of the points using the same scale for both the x-axis and the y-axis. It looks a little bit different than on the chart above.\\

\begin{figure}[htb]
\begin{center}
\includegraphics[width=8cm]{hw4pb2q2.jpg}
%\caption{}
\end{center}
\end{figure}

We denote:\\

z1=[0.1,0.5]    \#black\\
z2=[0.35,0.75]  \#black\\
z3=[0.28,1.35]  \#red\\
z4=[0,1.01]     \#blue\\

In bold in the lowest distance for the couple norm/distance, the clusters with the lowest distance will merge first.\\


\begin{tabular}{c c c c c c}

  Distance & Clusters & distance & $l_1$ & $l_2$ & $l_\infty$ \\
  \hline
  \hline
  SL & Blue, Black & d(z1,z4) & \textbf{0.61} & 0.5197 & 0.51 \\
  \hline
  SL & Blue, Red & d(z3,z4) & 0.62 & \textbf{0.4405} & \textbf{0.34} \\
  \hline
  SL & Black, Red & d(z2,z3) & 0.67 & 0.6041 & 0.6 \\
  \hline
  \hline
  CL & Blue, Black & d(z2,z4) & \textbf{0.61} & \textbf{0.436} & 0.35 \\
  \hline
  CL & Blue, Red & d(z3,z4) & 0.62 & 0.4405 & \textbf{0.34} \\
  \hline
  CL & Black, Red & d(z1,z3) & 1.03 & 0.8688 & 0.85 \\
  \hline
  \hline
  AL & Blue, Black & [d(z1,z4)+d(z2,z4)]/2 & \textbf{0.61} & 0.4778 & 0.43 \\
  \hline
  AL & Blue, Red & d(z3,z4) & 0.62 & \textbf{0.4405} & \textbf{0.34} \\
  \hline
  AL & Black, Red & [d(z1,z3)+d(z2,z3)]/2 & 0.85 & 0.7365  & 0.725\\
  \hline

\end{tabular}



\newpage

\section*{K-Means [15 pts]}
Implement K-Means clustering from scratch.\footnote{That is, don't use a
third-party machine learning implementation like \texttt{scikit-learn};
\texttt{numpy} is fine.}. You have been provided with the MNIST dataset. You can
learn more about it at  \url{http://yann.lecun.com/exdb/mnist/}. The MNIST task
is widely used in supervised learning, and modern algorithms with neural
networks do very well on this task. We can also use MNIST for interesting
unsupervised tasks. You are given representations of 6000 MNIST images, each of
which are $28\times28$  handwritten digits. In this problem, you will implement
K-means clustering on MNIST, to show how this relatively simple algorithm can
cluster similar-looking images together quite well.

\begin{problem}[K-means, 15pts]
The given code loads the images into your environment as a 6000x28x28 array.
Implement K-means clustering on it for a few different values of $K$, and show
results from the fit. Show the mean images for each class, and by selecting a
few representative images for each class. You should explain how you selected
these representative images. To render an image, use the numpy imshow function,
which the distribution code gives an example of. Use squared norm as your
distance metric. You should feel free to explore other metrics along with
squared norm if you are interested in seeing the effects of using those. Also,
your code should use the entire provided 6000-image dataset (which, by the way,
is only 10\% of the full MNIST set).

Are the results wildly different for different restarts and/or different $K$?
Plot the K-means objective function as a function of iteration and verify that
it never increases.

Finally, implement K-means++ and see if gives you more satisfying
initializations (and final results) for K-means. Explain your findings.

As in past problem sets, please include your plots in this document. There may
be tons of plots for this problem, so feel free to take up multiple pages, as
long as it is organized.
\end{problem}
\subsection*{Solution}

We implemented the K-means algorithm on the data set. Since the algorithm converges to a solution and stops improving I tweaked the algorithm so that it stops as soon as the loss function converged to a minimum. We gain computational time this way. On the chart below we see the loss decreases strictly in function of the iterations. It converges in average in around 40 steps for K=10.\\

\begin{figure}[htb]
\begin{center}
\includegraphics[width=8cm]{sumnorm100.png}
%\caption{}
\end{center}
\end{figure}

Since we are working with digits it made sense to start with K=10. Results were 'encouraging'. Unfortunately, the algorithm was not able to truly differentiate a 4 and a 5. Here are the mean images for the 10 clusters. Those mean images do not belong to the sample. Each image is define by 28*28 pixels and each pixel is a number for its darkness. The mean image is computed by taking the average of darkness for the 28*28 pixels. Since they are not actual handwritten image but averages, they are blurry. In the next pages, you will also find mean images for K=15 and 20. Finally, For K=10, we got 10 representative images for the first two clusters. We computed the distance to the nearest cluster and selected the 10 closest.\\

\begin{figure}[htp]
  \centering
  \label{figur}\caption{Mean images for K=10}

  \subfloat[]{\label{figur:1}\includegraphics[width=60mm]{kmean100_3}}
  \subfloat[]{\label{figur:2}\includegraphics[width=60mm]{kmean100_8}}
  \subfloat[]{\label{figur:3}\includegraphics[width=60mm]{kmean100_0}}
  \subfloat[]{\label{figur:4}\includegraphics[width=60mm]{kmean100_9}}
  \\
  \subfloat[]{\label{figur:5}\includegraphics[width=60mm]{kmean100_7}}
  \subfloat[]{\label{figur:6}\includegraphics[width=60mm]{kmean100_2}}
  \subfloat[]{\label{figur:5}\includegraphics[width=60mm]{kmean100_1}}
  \subfloat[]{\label{figur:6}\includegraphics[width=60mm]{kmean100_6}}
  \\
  \subfloat[]{\label{figur:5}\includegraphics[width=60mm]{kmean100_4}}
  \subfloat[]{\label{figur:6}\includegraphics[width=60mm]{kmean100_5}}
\end{figure}

\newpage

\begin{figure}[htp]
  \centering
  \label{figur}\caption{Mean images for K=15}

  \subfloat[]{\label{figur:1}\includegraphics[width=40mm]{kmean15_0}}
  \subfloat[]{\label{figur:2}\includegraphics[width=40mm]{kmean15_00}}
  \subfloat[]{\label{figur:3}\includegraphics[width=40mm]{kmean15_1}}
  \subfloat[]{\label{figur:4}\includegraphics[width=40mm]{kmean15_11}}
  \\
  \subfloat[]{\label{figur:5}\includegraphics[width=40mm]{kmean15_2}}
  \subfloat[]{\label{figur:6}\includegraphics[width=40mm]{kmean15_22}}
  \subfloat[]{\label{figur:7}\includegraphics[width=40mm]{kmean15_3}}
  \subfloat[]{\label{figur:8}\includegraphics[width=40mm]{kmean15_4}}
  \\
  \subfloat[]{\label{figur:9}\includegraphics[width=40mm]{kmean15_5}}
  \subfloat[]{\label{figur:10}\includegraphics[width=40mm]{kmean15_55}}
  \subfloat[]{\label{figur:11}\includegraphics[width=40mm]{kmean15_6}}
  \subfloat[]{\label{figur:12}\includegraphics[width=40mm]{kmean15_66}}
    \\
  \subfloat[]{\label{figur:14}\includegraphics[width=40mm]{kmean15_9}}
  \subfloat[]{\label{figur:13}\includegraphics[width=40mm]{kmean15_8}}
  \subfloat[]{\label{figur:15}\includegraphics[width=40mm]{kmean15_99}}
\end{figure}

\newpage

\begin{figure}[htp]
  \centering
  \label{figur}\caption{Mean images for K=20}

  \subfloat[]{\label{figur:11}\includegraphics[width=40mm]{kmean20_15}}
  \subfloat[]{\label{figur:1}\includegraphics[width=40mm]{kmean20_5}}
  \subfloat[]{\label{figur:1}\includegraphics[width=40mm]{kmean20_0}}
  \subfloat[]{\label{figur:1}\includegraphics[width=40mm]{kmean20_1}}
  \\
  \subfloat[]{\label{figur:14}\includegraphics[width=40mm]{kmean20_17}}
  \subfloat[]{\label{figur:10}\includegraphics[width=40mm]{kmean20_14}}
  \subfloat[]{\label{figur:7}\includegraphics[width=40mm]{kmean20_11}}
  \subfloat[]{\label{figur:3}\includegraphics[width=40mm]{kmean20_7}}
  \\
  \subfloat[]{\label{figur:2}\includegraphics[width=40mm]{kmean20_6}}
  \subfloat[]{\label{figur:13}\includegraphics[width=40mm]{kmean20_18}}
  \subfloat[]{\label{figur:6}\includegraphics[width=40mm]{kmean20_10}}
  \subfloat[]{\label{figur:8}\includegraphics[width=40mm]{kmean20_12}}
  \\
  \subfloat[]{\label{figur:5}\includegraphics[width=40mm]{kmean20_9}}
  \subfloat[]{\label{figur:1}\includegraphics[width=40mm]{kmean20_3}}
  \subfloat[]{\label{figur:1}\includegraphics[width=40mm]{kmean20_4}}
  \subfloat[]{\label{figur:15}\includegraphics[width=40mm]{kmean20_19}}
  \\
  \subfloat[]{\label{figur:4}\includegraphics[width=40mm]{kmean20_8}}
  \subfloat[]{\label{figur:9}\includegraphics[width=40mm]{kmean20_13}}
  \subfloat[]{\label{figur:12}\includegraphics[width=40mm]{kmean20_16}}
  \subfloat[]{\label{figur:1}\includegraphics[width=40mm]{kmean20_2}}
\end{figure}


\newpage


\begin{figure}[htp]
  \centering
  \label{figur}\caption{10 most representative images for the cluster '0'}

  \subfloat[]{\label{figur:11}\includegraphics[width=35mm]{represent0_1}}
  \subfloat[]{\label{figur:1}\includegraphics[width=35mm]{represent0_2}}
  \subfloat[]{\label{figur:1}\includegraphics[width=35mm]{represent0_3}}
  \subfloat[]{\label{figur:1}\includegraphics[width=35mm]{represent0_4}}
  \subfloat[]{\label{figur:14}\includegraphics[width=35mm]{represent0_5}}
  \\
  \subfloat[]{\label{figur:10}\includegraphics[width=35mm]{represent0_6}}
  \subfloat[]{\label{figur:7}\includegraphics[width=35mm]{represent0_7}}
  \subfloat[]{\label{figur:3}\includegraphics[width=35mm]{represent0_8}}
  \subfloat[]{\label{figur:2}\includegraphics[width=35mm]{represent0_9}}
  \subfloat[]{\label{figur:13}\includegraphics[width=35mm]{represent0_10}}
\end{figure}

\begin{figure}[htp]
  \centering
  \label{figur}\caption{10 most representative images for the cluster '1'}

  \subfloat[]{\label{figur:11}\includegraphics[width=35mm]{represent1_1}}
  \subfloat[]{\label{figur:1}\includegraphics[width=35mm]{represent1_2}}
  \subfloat[]{\label{figur:1}\includegraphics[width=35mm]{represent1_3}}
  \subfloat[]{\label{figur:1}\includegraphics[width=35mm]{represent1_4}}
  \subfloat[]{\label{figur:14}\includegraphics[width=35mm]{represent1_5}}
  \\
  \subfloat[]{\label{figur:10}\includegraphics[width=35mm]{represent1_6}}
  \subfloat[]{\label{figur:7}\includegraphics[width=35mm]{represent1_7}}
  \subfloat[]{\label{figur:3}\includegraphics[width=35mm]{represent1_8}}
  \subfloat[]{\label{figur:2}\includegraphics[width=35mm]{represent1_9}}
  \subfloat[]{\label{figur:13}\includegraphics[width=35mm]{represent1_10}}
\end{figure}




\newpage

We finish with the K++ initialization. Below you will find the 10 images that will be used for the initialization of the K-mean algo. We ran it for K=10. We notice several '3' in this example but after a few iterations the algorithm lead to a clustering similar to what a simple K-mean would have given. Since the initialization provides already very "different" images in input, the clustering is faster and lead to results at least as good as the simple version.\\

\begin{figure}[htp]
  \centering
  \label{figur}\caption{10 images for the initialization of the K-mean algo (K-Mean++)}

  \subfloat[]{\label{figur:11}\includegraphics[width=35mm]{plusplus_1}}
  \subfloat[]{\label{figur:1}\includegraphics[width=35mm]{plusplus_2}}
  \subfloat[]{\label{figur:1}\includegraphics[width=35mm]{plusplus_3}}
  \subfloat[]{\label{figur:1}\includegraphics[width=35mm]{plusplus_4}}
  \subfloat[]{\label{figur:14}\includegraphics[width=35mm]{plusplus_5}}
  \\
  \subfloat[]{\label{figur:10}\includegraphics[width=35mm]{plusplus_6}}
  \subfloat[]{\label{figur:7}\includegraphics[width=35mm]{plusplus_7}}
  \subfloat[]{\label{figur:3}\includegraphics[width=35mm]{plusplus_8}}
  \subfloat[]{\label{figur:2}\includegraphics[width=35mm]{plusplus_9}}
  \subfloat[]{\label{figur:13}\includegraphics[width=35mm]{plusplus_0}}
\end{figure}


Are the results wildly different for different restarts and/or different $K$?\\
No results are relatively similar.\\

Plot the K-means objective function as a function of iteration and verify that
it never increases.\\

done. see above.\\

Finally, implement K-means++ and see if gives you more satisfying
initializations (and final results) for K-means. Explain your findings.




\newpage
\begin{problem}[Calibration, 1pt]
Approximately how long did this homework take you to complete? 10h
\end{problem}


\end{document}
